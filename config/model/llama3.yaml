_target_: bit_llm.model.Llama

vocab_size: 128256
hidden_dim: 4096
intermediate_size: 14336
num_layers: 32
num_attention_heads: 32
num_key_value_heads: 8
rope_theta: 500000